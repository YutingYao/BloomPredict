{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但现在我们需要制作了一个新的csv：df = pd.read_csv('/root/Download/AlgaeBloomForecast/merged_data.csv')\n",
    "\n",
    "```\n",
    "date,temp,oxygen,NH3,TP,TN,algae,area,weather,max_temperature,min_temperature,aqi,aqiLevel,wind_direction,wind_power,aqiInfo\n",
    "2021-06-02,26.1875,6.6665,0.025,0.068275,1.07325,14400000.0,无锡,阴-阵雨,26,21,24,1,东南风,4级,优\n",
    "2021-06-03,25.881666666666664,6.6418333333333335,0.0251166666666666,0.0637833333333333,0.9151666666666666,10867091.666666666,无锡,阴-阵雨,26,19,66,2,西北风,3级,良\n",
    "2021-06-04,25.895,7.946333333333333,0.025,0.0637833333333333,0.9203333333333332,25498423.33333333,无锡,阴-多云,26,18,51,2,西南风,3级,良\n",
    "2021-06-05,26.85,9.084,0.025,0.04776,0.9058,21100000.0,无锡,晴,32,19,67,2,西南风,3级,良\n",
    "2021-06-06,28.256666666666664,9.514333333333331,0.025,0.0440666666666666,0.9233333333333332,15211340.0,无锡,晴,33,19,80,2,南风,3级,良\n",
    "2021-06-07,27.635,8.3865,0.025,0.0366499999999999,0.7778333333333333,7994458.333333333,无锡,阴-多云,35,21,68,2,东南风,3级,良\n",
    "2021-06-08,28.19666666666667,8.397499999999999,0.025,0.0418666666666666,0.7323333333333334,12259158.333333334,无锡,阴-多云,30,24,36,1,东南风,3级,优\n",
    "2021-06-09,28.751666666666665,8.309166666666668,0.025,0.0389833333333333,0.601,6891956.666666667,无锡,阴-雷阵雨,32,24,52,2,东南风,3级,良\n",
    "2021-06-10,28.741666666666664,7.385833333333333,0.025,0.03785,0.5256666666666666,6301236.666666667,无锡,阴,28,24,38,1,东南风,2级,优\n",
    "2021-06-11,29.491666666666664,7.6176666666666675,0.025,0.0327666666666666,0.4495,6244151.666666667,无锡,阴-多云,32,23,82,2,东风,2级,良\n",
    "2021-06-12,29.58666666666667,7.271999999999999,0.025,0.02975,0.3741666666666667,4201731.666666667,无锡,多云-雷阵雨,33,24,41,1,东南风,3级,优\n",
    "2021-06-13,29.563333333333333,6.929333333333333,0.025,0.0302833333333333,0.2663333333333333,4964940.0,无锡,阴-小雨,28,25,34,1,西南风,2级,优\n",
    "2021-06-14,29.58833333333333,6.963166666666666,0.025,0.0290666666666666,0.1886666666666666,5394340.0,无锡,阴-小雨,31,25,46,1,东南风,3级,优\n",
    "2021-06-15,30.21,7.23925,0.025,0.033425,0.396,6927237.5,无锡,阴-小雨,33,24,48,1,西南风,3级,优\n",
    "```\n",
    "\n",
    "这里我们要用Transformer预测藻类的爆发，来捕获时间序列数据的长距离依赖关系。这里提供了逐日的数据，需要考虑date的影响。\n",
    "\n",
    "通过前期的数据分析，我们发现：\n",
    "- 这里需要考虑['temp', 'oxygen', 'NH3', 'TP', 'TN']对'algae'的影响\n",
    "- 这里需要考虑weather这一列出现“晴”的影响，以及“晴”的长距离依赖关系。\n",
    "- 温度的影响，这里，只采用temp作为特征，考虑温度的长距离依赖关系。\n",
    "- 不考虑max_temperature,min_temperature。\n",
    "- wind_power大于4级时是一个需要考虑的因素，考虑wind_power的长距离依赖关系。\n",
    "- 这里的aqi,aqiLevel，aqiInfo与空气质量相关，不考虑。\n",
    "- wind_direction也不考虑。\n",
    "\n",
    "请你输出完整的代码。需要：\n",
    "- 做归一化的处理\n",
    "- 需要对数据分割\n",
    "- 需要绘制训练过程的图片、需要输出预测效果的图片\n",
    "- 模型的训练参数是可调整的，比如，添加了学习率调度（ReduceLROnPlateau）、在优化器中添加了L2正则化（weight_decay）、调整了模型结构，包括隐藏层维度和dropout率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我可以帮你修改代码以使用过去20天的数据来预测下一天。这个改变主要涉及序列长度的调整，以及可能的模型结构优化。以下是修改后的代码：\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "主要的变化和优化包括：\n",
    "\n",
    "1. 序列长度从 7 天增加到 4 天：\n",
    "   ```python\n",
    "   sequence_length = 4  # 使用过去20天的数据预测下一天\n",
    "   ```\n",
    "\n",
    "2. 增加了位置编码（Positional Encoding）:\n",
    "   这有助于模型理解序列中每个元素的相对位置。\n",
    "\n",
    "3. 调整了模型参数：\n",
    "   - 增加了隐藏层维度（从 64 到 128）\n",
    "   - 增加了 Transformer 层数（从 3 到 4）\n",
    "   - 增加了注意力头数（从 1 到 8）\n",
    "   - 稍微增加了 dropout（从 0.1 到 0.2）\n",
    "\n",
    "4. 在 Transformer 模型中使用了位置编码：\n",
    "   ```python\n",
    "   self.pos_encoder = PositionalEncoding(self.embed_dim, dropout)\n",
    "   ```\n",
    "\n",
    "5. 增加了前馈网络的维度：\n",
    "   ```python\n",
    "   dim_feedforward=hidden_dim * 4\n",
    "   ```\n",
    "\n",
    "这些修改应该能够帮助模型更好地处理更长的序列（20天），并可能提高预测性能。但是，你可能需要根据实际运行结果进行进一步的调整。\n",
    "\n",
    "如果你在运行这段代码时遇到任何问题，或者想要进行更多的优化，请随时告诉我。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_27119/3985408754.py:17: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['wind_power_high'] = (df['wind_power'].str.extract('(\\d+)').astype(int) > 4).astype(int)\n",
      "/root/.conda/envs/hydro/envs/algae/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.0759, Test Loss: 0.0399\n",
      "Epoch 2/100, Train Loss: 0.0294, Test Loss: 0.0195\n",
      "Epoch 3/100, Train Loss: 0.0249, Test Loss: 0.0176\n",
      "Epoch 4/100, Train Loss: 0.0215, Test Loss: 0.0163\n",
      "Epoch 5/100, Train Loss: 0.0222, Test Loss: 0.0167\n",
      "Epoch 6/100, Train Loss: 0.0207, Test Loss: 0.0194\n",
      "Epoch 7/100, Train Loss: 0.0203, Test Loss: 0.0152\n",
      "Epoch 8/100, Train Loss: 0.0201, Test Loss: 0.0196\n",
      "Epoch 9/100, Train Loss: 0.0196, Test Loss: 0.0193\n",
      "Epoch 10/100, Train Loss: 0.0185, Test Loss: 0.0189\n",
      "Epoch 11/100, Train Loss: 0.0189, Test Loss: 0.0163\n",
      "Epoch 12/100, Train Loss: 0.0195, Test Loss: 0.0196\n",
      "Epoch 13/100, Train Loss: 0.0178, Test Loss: 0.0158\n",
      "Epoch 14/100, Train Loss: 0.0181, Test Loss: 0.0163\n",
      "Epoch 15/100, Train Loss: 0.0183, Test Loss: 0.0151\n",
      "Epoch 16/100, Train Loss: 0.0179, Test Loss: 0.0172\n",
      "Epoch 17/100, Train Loss: 0.0177, Test Loss: 0.0187\n",
      "Epoch 18/100, Train Loss: 0.0179, Test Loss: 0.0270\n",
      "Epoch 19/100, Train Loss: 0.0211, Test Loss: 0.0161\n",
      "Epoch 20/100, Train Loss: 0.0169, Test Loss: 0.0184\n",
      "Epoch 21/100, Train Loss: 0.0173, Test Loss: 0.0195\n",
      "Epoch 22/100, Train Loss: 0.0175, Test Loss: 0.0179\n",
      "Epoch 23/100, Train Loss: 0.0170, Test Loss: 0.0158\n",
      "Epoch 24/100, Train Loss: 0.0166, Test Loss: 0.0173\n",
      "Epoch 25/100, Train Loss: 0.0170, Test Loss: 0.0168\n",
      "Epoch 26/100, Train Loss: 0.0165, Test Loss: 0.0172\n",
      "Epoch 27/100, Train Loss: 0.0164, Test Loss: 0.0172\n",
      "Epoch 28/100, Train Loss: 0.0162, Test Loss: 0.0179\n",
      "Epoch 29/100, Train Loss: 0.0165, Test Loss: 0.0172\n",
      "Epoch 30/100, Train Loss: 0.0167, Test Loss: 0.0171\n",
      "Epoch 31/100, Train Loss: 0.0163, Test Loss: 0.0174\n",
      "Epoch 32/100, Train Loss: 0.0163, Test Loss: 0.0183\n",
      "Epoch 33/100, Train Loss: 0.0160, Test Loss: 0.0182\n",
      "Epoch 34/100, Train Loss: 0.0165, Test Loss: 0.0172\n",
      "Epoch 35/100, Train Loss: 0.0164, Test Loss: 0.0183\n",
      "Epoch 36/100, Train Loss: 0.0162, Test Loss: 0.0163\n",
      "Epoch 37/100, Train Loss: 0.0176, Test Loss: 0.0176\n",
      "Epoch 38/100, Train Loss: 0.0160, Test Loss: 0.0173\n",
      "Epoch 39/100, Train Loss: 0.0164, Test Loss: 0.0170\n",
      "Epoch 40/100, Train Loss: 0.0166, Test Loss: 0.0170\n",
      "Epoch 41/100, Train Loss: 0.0158, Test Loss: 0.0171\n",
      "Epoch 42/100, Train Loss: 0.0163, Test Loss: 0.0172\n",
      "Epoch 43/100, Train Loss: 0.0170, Test Loss: 0.0171\n",
      "Epoch 44/100, Train Loss: 0.0174, Test Loss: 0.0170\n",
      "Epoch 45/100, Train Loss: 0.0164, Test Loss: 0.0170\n",
      "Epoch 46/100, Train Loss: 0.0165, Test Loss: 0.0171\n",
      "Epoch 47/100, Train Loss: 0.0162, Test Loss: 0.0171\n",
      "Epoch 48/100, Train Loss: 0.0164, Test Loss: 0.0170\n",
      "Epoch 49/100, Train Loss: 0.0160, Test Loss: 0.0170\n",
      "Epoch 50/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 51/100, Train Loss: 0.0165, Test Loss: 0.0170\n",
      "Epoch 52/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 53/100, Train Loss: 0.0168, Test Loss: 0.0170\n",
      "Epoch 54/100, Train Loss: 0.0162, Test Loss: 0.0170\n",
      "Epoch 55/100, Train Loss: 0.0168, Test Loss: 0.0170\n",
      "Epoch 56/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 57/100, Train Loss: 0.0160, Test Loss: 0.0170\n",
      "Epoch 58/100, Train Loss: 0.0164, Test Loss: 0.0170\n",
      "Epoch 59/100, Train Loss: 0.0172, Test Loss: 0.0170\n",
      "Epoch 60/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 61/100, Train Loss: 0.0171, Test Loss: 0.0170\n",
      "Epoch 62/100, Train Loss: 0.0170, Test Loss: 0.0170\n",
      "Epoch 63/100, Train Loss: 0.0163, Test Loss: 0.0170\n",
      "Epoch 64/100, Train Loss: 0.0165, Test Loss: 0.0170\n",
      "Epoch 65/100, Train Loss: 0.0166, Test Loss: 0.0170\n",
      "Epoch 66/100, Train Loss: 0.0167, Test Loss: 0.0170\n",
      "Epoch 67/100, Train Loss: 0.0163, Test Loss: 0.0170\n",
      "Epoch 68/100, Train Loss: 0.0157, Test Loss: 0.0170\n",
      "Epoch 69/100, Train Loss: 0.0167, Test Loss: 0.0170\n",
      "Epoch 70/100, Train Loss: 0.0177, Test Loss: 0.0170\n",
      "Epoch 71/100, Train Loss: 0.0165, Test Loss: 0.0170\n",
      "Epoch 72/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 73/100, Train Loss: 0.0162, Test Loss: 0.0170\n",
      "Epoch 74/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 75/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 76/100, Train Loss: 0.0159, Test Loss: 0.0170\n",
      "Epoch 77/100, Train Loss: 0.0167, Test Loss: 0.0170\n",
      "Epoch 78/100, Train Loss: 0.0162, Test Loss: 0.0170\n",
      "Epoch 79/100, Train Loss: 0.0157, Test Loss: 0.0170\n",
      "Epoch 80/100, Train Loss: 0.0164, Test Loss: 0.0170\n",
      "Epoch 81/100, Train Loss: 0.0163, Test Loss: 0.0170\n",
      "Epoch 82/100, Train Loss: 0.0158, Test Loss: 0.0170\n",
      "Epoch 83/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 84/100, Train Loss: 0.0165, Test Loss: 0.0170\n",
      "Epoch 85/100, Train Loss: 0.0163, Test Loss: 0.0170\n",
      "Epoch 86/100, Train Loss: 0.0159, Test Loss: 0.0170\n",
      "Epoch 87/100, Train Loss: 0.0160, Test Loss: 0.0170\n",
      "Epoch 88/100, Train Loss: 0.0165, Test Loss: 0.0170\n",
      "Epoch 89/100, Train Loss: 0.0162, Test Loss: 0.0170\n",
      "Epoch 90/100, Train Loss: 0.0177, Test Loss: 0.0170\n",
      "Epoch 91/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 92/100, Train Loss: 0.0163, Test Loss: 0.0170\n",
      "Epoch 93/100, Train Loss: 0.0167, Test Loss: 0.0170\n",
      "Epoch 94/100, Train Loss: 0.0174, Test Loss: 0.0170\n",
      "Epoch 95/100, Train Loss: 0.0162, Test Loss: 0.0170\n",
      "Epoch 96/100, Train Loss: 0.0169, Test Loss: 0.0170\n",
      "Epoch 97/100, Train Loss: 0.0161, Test Loss: 0.0170\n",
      "Epoch 98/100, Train Loss: 0.0156, Test Loss: 0.0170\n",
      "Epoch 99/100, Train Loss: 0.0160, Test Loss: 0.0170\n",
      "Epoch 100/100, Train Loss: 0.0163, Test Loss: 0.0170\n",
      "Mean Squared Error: 1121580439623941.5000\n",
      "Mean Absolute Error: 24103364.0367\n",
      "R-squared: -0.2473\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('/root/Download/AlgaeBloomForecast/merged_data.csv')\n",
    "\n",
    "# 数据预处理\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['is_sunny'] = df['weather'].str.contains('晴').astype(int)\n",
    "df['wind_power_high'] = (df['wind_power'].str.extract('(\\d+)').astype(int) > 4).astype(int)\n",
    "\n",
    "# 选择特征\n",
    "features = ['temp', 'oxygen', 'NH3', 'TP', 'TN', 'is_sunny', 'wind_power_high']\n",
    "target = 'algae'\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler()\n",
    "df[features + [target]] = scaler.fit_transform(df[features + [target]])\n",
    "\n",
    "# 准备序列数据\n",
    "sequence_length = 4  # 使用过4天的数据预测下一天\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# 按时间顺序排序数据\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# 选择最后3个月的数据作为测试集\n",
    "test_size = 180  # 假设每月30天\n",
    "train_data = df[:-test_size]\n",
    "test_data = df[-test_size:]\n",
    "\n",
    "# 创建训练集和测试集序列\n",
    "X_train, y_train = create_sequences(train_data[features + [target]].values, sequence_length)\n",
    "X_test, y_test = create_sequences(test_data[features + [target]].values, sequence_length)\n",
    "\n",
    "X_train = X_train[:, :, :-1]  # 移除目标变量\n",
    "y_train = y_train[:, -1]  # 只保留目标变量\n",
    "X_test = X_test[:, :, :-1]\n",
    "y_test = y_test[:, -1]\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 修改 Transformer 模型以适应更长的序列\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = hidden_dim\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, self.embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.embed_dim, nhead=num_heads, \n",
    "                                                    dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(self.embed_dim, output_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src shape: [seq_len, batch_size, input_dim]\n",
    "        src = self.input_projection(src)  # [seq_len, batch_size, embed_dim]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)  # [seq_len, batch_size, embed_dim]\n",
    "        output = output[-1, :, :]  # 取最后一个时间步 [batch_size, embed_dim]\n",
    "        output = self.decoder(output)  # [batch_size, output_dim]\n",
    "        return output.squeeze(-1)  # [batch_size]\n",
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 设置模型参数\n",
    "input_dim = len(features)\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "num_layers = 3\n",
    "num_heads = 1\n",
    "dropout = 0.1\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = TransformerModel(input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X.permute(1, 0, 2))\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X.permute(1, 0, 2))\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# 绘制训练过程\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "plt.savefig('training_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# 预测\n",
    "model.eval()\n",
    "predictions = []\n",
    "actual = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X.permute(1, 0, 2))\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actual.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# 反归一化预测结果和实际值\n",
    "predictions = scaler.inverse_transform(np.column_stack((np.zeros((len(predictions), len(features))), np.array(predictions))))[:, -1]\n",
    "actual = scaler.inverse_transform(np.column_stack((np.zeros((len(actual), len(features))), np.array(actual))))[:, -1]\n",
    "\n",
    "# 绘制预测效果图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_data['date'][-len(actual):], actual, label='Actual')\n",
    "plt.plot(test_data['date'][-len(predictions):], predictions, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Algae Bloom')\n",
    "plt.title('Actual vs Predicted Algae Bloom (Last 3 Months)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_results_time_series.png')\n",
    "plt.close()\n",
    "\n",
    "# 计算和打印评估指标\n",
    "mse = np.mean((predictions - actual) ** 2)\n",
    "mae = np.mean(np.abs(predictions - actual))\n",
    "r2 = 1 - (np.sum((actual - predictions) ** 2) / np.sum((actual - np.mean(actual)) ** 2))\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了提高预测的准确性，我们可以从多个方面对代码进行微调。以下是一些建议：\n",
    "\n",
    "1. 特征工程：\n",
    "   - 添加更多相关特征，如滞后特征（lag features）或滚动统计特征。\n",
    "   - 创建交互特征。\n",
    "\n",
    "```python\n",
    "# 添加滞后特征\n",
    "for feature in features:\n",
    "    for lag in [1, 2, 3]:  # 可以根据需要调整滞后天数\n",
    "        df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "\n",
    "# 添加滚动统计特征\n",
    "for feature in features:\n",
    "    df[f'{feature}_rolling_mean_7d'] = df[feature].rolling(window=7).mean()\n",
    "    df[f'{feature}_rolling_std_7d'] = df[feature].rolling(window=7).std()\n",
    "\n",
    "# 更新特征列表\n",
    "features = [col for col in df.columns if col not in ['date', 'algae', 'area', 'weather']]\n",
    "```\n",
    "\n",
    "2. 调整模型架构：\n",
    "   - 增加模型的复杂度，如增加层数或隐藏单元数。\n",
    "   - 尝试不同的激活函数。\n",
    "\n",
    "```python\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = hidden_dim\n",
    "\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, \n",
    "                                                        dim_feedforward=hidden_dim*4, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output[-1, :, :]\n",
    "        output = self.decoder(output)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "# 调整模型参数\n",
    "hidden_dim = 128\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "dropout = 0.2\n",
    "```\n",
    "\n",
    "3. 调整训练过程：\n",
    "   - 使用学习率衰减策略。\n",
    "   - 增加训练轮数。\n",
    "   - 实现早停（Early Stopping）。\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "num_epochs = 200\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ... (训练代码)\n",
    "    \n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "4. 数据增强：\n",
    "   - 对于时间序列数据，可以使用滑动窗口方法创建更多训练样本。\n",
    "\n",
    "```python\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# 使用更短的序列长度创建更多样本\n",
    "sequence_length = 5\n",
    "X, y = create_sequences(df[features + [target]].values, sequence_length)\n",
    "```\n",
    "\n",
    "5. 集成方法：\n",
    "   - 训练多个模型并集成它们的预测结果。\n",
    "\n",
    "```python\n",
    "num_models = 5\n",
    "models = [TransformerModel(input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout) for _ in range(num_models)]\n",
    "\n",
    "# 训练每个模型...\n",
    "\n",
    "# 预测时使用所有模型的平均预测\n",
    "def ensemble_predict(models, X):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X.permute(1, 0, 2))\n",
    "        predictions.append(pred.cpu().numpy())\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "ensemble_predictions = ensemble_predict(models, X_test)\n",
    "```\n",
    "\n",
    "6. 交叉验证：\n",
    "   - 使用交叉验证来更好地评估模型性能并防止过拟合。\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    # 训练和评估模型...\n",
    "```\n",
    "\n",
    "这些建议可以单独使用或组合使用。建议您逐步实施这些更改，并密切监控模型的性能。每次更改后，都要评估模型在验证集上的表现，以确保更改确实提高了预测准确性。\n",
    "\n",
    "如果您想实施其中的任何建议，或者需要更详细的解释，请随时告诉我。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
